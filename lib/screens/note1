// lib/screens/face_recognition_screen.dart
import 'dart:async';
import 'dart:convert';
import 'dart:math';
import 'dart:typed_data';
import 'dart:ui' as ui;

import 'package:flutter/material.dart';
import 'package:camera/camera.dart';
import 'package:flutter/services.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:audioplayers/audioplayers.dart';
import 'package:geolocator/geolocator.dart';
import 'package:http/http.dart' as http;

class FaceRecognitionScreen extends StatefulWidget {
  const FaceRecognitionScreen({super.key});

  @override
  State<FaceRecognitionScreen> createState() => _FaceRecognitionScreenState();
}

class _FaceRecognitionScreenState extends State<FaceRecognitionScreen> with WidgetsBindingObserver {
  CameraController? _controller;
  late Future<void> _initializeControllerFuture;

  final FaceDetector _faceDetector = FaceDetector(
    options: FaceDetectorOptions(
      performanceMode: FaceDetectorMode.accurate,
      enableLandmarks: false,
      enableContours: false,
      enableClassification: false,
    ),
  );

  final AudioPlayer _audioPlayer = AudioPlayer();

  bool _recognizing = false;
  int _cooldownMs = 2500;
  int _lastRecognition = 0;

  // small UI state
  Uint8List? _capturedFaceBytes;
  String? _imageUrl;
  String _status = 'Initializing camera...';
  String _userId = '';
  String _userName = '';

  // geofence
  final double _geoLat = 12.997666;
  final double _geoLng = 77.669542;
  final double _geoRadiusM = 60.0;

  @override
  void initState() {
    super.initState();
    WidgetsBinding.instance.addObserver(this);
    _initCamera();
  }

  @override
  void didChangeAppLifecycleState(AppLifecycleState state) {
    if (_controller == null || !_controller!.value.isInitialized) return;
    if (state == AppLifecycleState.inactive) {
      _controller?.dispose();
    } else if (state == AppLifecycleState.resumed) {
      _initCamera();
    }
  }

  Future<void> _initCamera() async {
    try {
      final cameras = await availableCameras();
      final front = cameras.firstWhere(
        (c) => c.lensDirection == CameraLensDirection.front,
        orElse: () => cameras.first,
      );

      _controller = CameraController(
        front,
        ResolutionPreset.high,
        enableAudio: false,
        imageFormatGroup: ImageFormatGroup.bgra8888, // critical: use BGRA for ML Kit stability
      );

      _initializeControllerFuture = _controller!.initialize();
      await _initializeControllerFuture;

      if (!mounted) return;

      // start stream
      _controller!.startImageStream(_processCameraImage);

      setState(() => _status = 'Camera ready ✅');
    } catch (e) {
      setState(() => _status = 'Camera init error: $e');
    }
  }

  // Map sensor orientation (degrees) -> InputImageRotation
  InputImageRotation _rotationFromSensor(int sensorOrientation) {
    switch (sensorOrientation) {
      case 0:
        return InputImageRotation.rotation0deg;
      case 90:
        return InputImageRotation.rotation90deg;
      case 180:
        return InputImageRotation.rotation180deg;
      case 270:
        return InputImageRotation.rotation270deg;
      default:
        return InputImageRotation.rotation0deg;
    }
  }

  Future<void> _processCameraImage(CameraImage image) async {
    if (_recognizing) return;

    final now = DateTime.now().millisecondsSinceEpoch;
    if (now - _lastRecognition < _cooldownMs) return;

    // must have controller and initialized
    if (_controller == null || !_controller!.value.isInitialized) return;

    try {
      // IMPORTANT: handle format differences: for BGRA, there is single plane
      final formatGroup = _controller!.value.description.sensorOrientation; // sensorOrientation for rotation
      final rotation = _rotationFromSensor(_controller!.description.sensorOrientation);

      // For bgra8888, pass the first plane's bytes directly (no concatenation)
      Uint8List bytesForMl;
      if (image.format.group == ImageFormatGroup.bgra8888 && image.planes.isNotEmpty) {
        // ML Kit expects the bytes (BGRA) with proper metadata; pass as-is
        bytesForMl = image.planes[0].bytes;
      } else {
        // fallback (shouldn't happen because we set bgra), concat planes
        final buffer = WriteBuffer();
        for (var p in image.planes) buffer.putUint8List(p.bytes);
        bytesForMl = buffer.done().buffer.asUint8List();
      }

      final inputImage = InputImage.fromBytes(
        bytes: bytesForMl,
        metadata: InputImageMetadata(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          rotation: rotation,
          format: InputImageFormat.bgra8888,
          bytesPerRow: image.planes[0].bytesPerRow,
        ),
      );

      // run ML Kit face detector (may throw PlatformException if metadata is wrong)
      final faces = await _faceDetector.processImage(inputImage);

      if (faces.isEmpty) {
        // no face — keep placeholder
        if (mounted) {
          setState(() {
            _capturedFaceBytes = null;
            _imageUrl = null;
            _status = 'No face detected';
          });
        }
        return;
      }

      // pick largest face
      final face = faces.reduce((a, b) {
        final areaA = a.boundingBox.width * a.boundingBox.height;
        final areaB = b.boundingBox.width * b.boundingBox.height;
        return areaA > areaB ? a : b;
      });

      // small head-pose filter like Kotlin (yaw and roll)
      final yaw = face.headEulerAngleY ?? 0.0;
      final roll = face.headEulerAngleZ ?? 0.0;
      if (yaw.abs() > 15 || roll.abs() > 15) {
        // face too rotated — skip but keep placeholder
        if (mounted) {
          setState(() {
            _status = 'Please face camera straight';
          });
        }
        return;
      }

      // Crop & encode the detected face (160x160)
      final cropped = await _cropAndEncodeFromBgra(image, face.boundingBox);
      if (cropped == null) {
        return;
      }

      // show captured face and start recognition
      if (mounted) {
        setState(() {
          _capturedFaceBytes = cropped;
          _imageUrl = null;
          _status = 'Recognizing...';
        });
      }

      // geofence check
      final pos = await Geolocator.getCurrentPosition(desiredAccuracy: LocationAccuracy.high);
      final inside = _isInsideGeofence(pos.latitude, pos.longitude);
      if (!inside) {
        if (mounted) {
          setState(() {
            _status = 'Outside Geofence ❌';
          });
        }
        _playSoundDenied();
        return;
      }

      _recognizing = true;

      // send to backend
      final base64Image = base64Encode(cropped); // PNG bytes
      final result = await _sendToServer(base64Image, pos.latitude, pos.longitude);

      _lastRecognition = DateTime.now().millisecondsSinceEpoch;
      _recognizing = false;

      if (result['matched'] == true) {
        if (mounted) {
          setState(() {
            _userId = result['user_id']?.toString() ?? '';
            _userName = result['name']?.toString() ?? '';
            _status = 'Access Granted ✅';
            // many servers return face_image_url list or single string
            _imageUrl = (result['face_image_url'] is List) ? result['face_image_url'][0] : result['face_image_url'];
          });
        }
        _playSoundGranted();
      } else {
        if (mounted) {
          setState(() {
            _status = result['message']?.toString() ?? 'Face not recognized ❌';
            _userId = '';
            _userName = '';
            // fallback to placeholder after a short delay
          });
        }
        _playSoundDenied();
      }

      // reset small UI after cooldown
      Future.delayed(Duration(milliseconds: _cooldownMs), () {
        if (mounted) {
          setState(() {
            _status = '';
            _capturedFaceBytes = null;
            _imageUrl = null;
            _userId = '';
            _userName = '';
          });
        }
      });
    } on PlatformException catch (e) {
      // InputImage conversion failed — show placeholder and continue
      if (mounted) {
        setState(() {
          _status = 'Face detection error';
          _capturedFaceBytes = null;
          _imageUrl = null;
        });
      }
    } catch (e) {
      if (mounted) {
        setState(() {
          _status = 'Error: $e';
          _capturedFaceBytes = null;
          _imageUrl = null;
        });
      }
    }
  }

  // Convert BGRA plane -> RGBA bytes and crop+resize to 160x160 using ui
  Future<Uint8List?> _cropAndEncodeFromBgra(CameraImage image, Rect bbox) async {
    try {
      // image.planes[0].bytes is BGRA
      final bgra = image.planes[0].bytes;
      final width = image.width;
      final height = image.height;

      // Convert BGRA -> RGBA (ML/decoding expects RGBA8888 for decodeImageFromPixels)
      final rgba = Uint8List(bgra.length);
      for (int i = 0; i < bgra.length; i += 4) {
        final b = bgra[i];
        final g = bgra[i + 1];
        final r = bgra[i + 2];
        final a = bgra[i + 3];
        rgba[i] = r;
        rgba[i + 1] = g;
        rgba[i + 2] = b;
        rgba[i + 3] = a;
      }

      // Decode to ui.Image
      final completer = Completer<ui.Image>();
      ui.decodeImageFromPixels(rgba, width, height, ui.PixelFormat.rgba8888, (ui.Image img) {
        completer.complete(img);
      });
      final fullImage = await completer.future;

      // Compute square crop with margin, clamp to image bounds
      final margin = 0.30 * max(bbox.width, bbox.height);
      int left = max(0, (bbox.left - margin).round());
      int top = max(0, (bbox.top - margin).round());
      int right = min(width, (bbox.right + margin).round());
      int bottom = min(height, (bbox.bottom + margin).round());

      int size = max(right - left, bottom - top);
      final cx = ((left + right) / 2).round();
      final cy = ((top + bottom) / 2).round();
      left = max(0, cx - (size ~/ 2));
      top = max(0, cy - (size ~/ 2));
      right = min(width, left + size);
      bottom = min(height, top + size);

      // Prepare source and destination rects
      final srcRect = Rect.fromLTWH(left.toDouble(), top.toDouble(), (right - left).toDouble(), (bottom - top).toDouble());
      final dstRect = const Rect.fromLTWH(0.0, 0.0, 160.0, 160.0);

      // Draw into a 160x160 canvas — mirror horizontally since front camera preview is mirrored
      final recorder = ui.PictureRecorder();
      final canvas = Canvas(recorder);
      final paint = Paint();

      // Mirror horizontally so captured face appears natural (same as Kotlin behavior)
      canvas.translate(160.0, 0.0);
      canvas.scale(-1.0, 1.0);

      canvas.drawImageRect(fullImage, srcRect, dstRect, paint);

      final picture = recorder.endRecording();
      final cropped = await picture.toImage(160, 160);
      final bytes = await cropped.toByteData(format: ui.ImageByteFormat.png);
      return bytes?.buffer.asUint8List();
    } catch (e) {
      // any error -> return null
      debugPrint('crop error: $e');
      return null;
    }
  }

  bool _isInsideGeofence(double lat, double lng) {
    final distance = Geolocator.distanceBetween(lat, lng, _geoLat, _geoLng);
    return distance <= _geoRadiusM;
  }

  Future<Map<String, dynamic>> _sendToServer(String base64Png, double lat, double lng) async {
    try {
      final resp = await http.post(
        Uri.parse('http://192.168.5.110:5000/recognize'),
        headers: {'Content-Type': 'application/json'},
        body: jsonEncode({
          'face_image': 'data:image/png;base64,$base64Png',
          'latitude': lat,
          'longitude': lng,
        }),
      );

      if (resp.statusCode == 200) {
        return jsonDecode(resp.body) as Map<String, dynamic>;
      } else {
        return {'matched': false, 'message': 'Server error: ${resp.statusCode}'};
      }
    } catch (e) {
      return {'matched': false, 'message': 'Network error'};
    }
  }

  Future<void> _playSoundGranted() async {
    try {
      await _audioPlayer.play(AssetSource('assets/sounds/access_granted.mp3'));
    } catch (_) {}
  }

  Future<void> _playSoundDenied() async {
    try {
      await _audioPlayer.play(AssetSource('assets/sounds/access_denied.mp3'));
    } catch (_) {}
  }

  @override
  void dispose() {
    WidgetsBinding.instance.removeObserver(this);
    _controller?.dispose();
    _faceDetector.close();
    _audioPlayer.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    final screenHeight = MediaQuery.of(context).size.height;

    return Scaffold(
      backgroundColor: Colors.black,
      body: FutureBuilder(
        future: _controller?.initialize(),
        builder: (context, snapshot) {
          if (snapshot.connectionState == ConnectionState.done && _controller != null && _controller!.value.isInitialized) {
            return Stack(
              children: [
                // Camera preview covering 80% height
                Positioned(
                  top: 0,
                  left: 0,
                  right: 0,
                  child: SizedBox(
                    height: screenHeight * 0.8,
                    child: CameraPreview(_controller!),
                  ),
                ),

                // Info panel bottom-left
                Positioned(
                  bottom: 30,
                  left: 20,
                  child: Row(
                    crossAxisAlignment: CrossAxisAlignment.center,
                    children: [
                      // face / placeholder thumbnail
                      Container(
                        width: 90,
                        height: 90,
                        decoration: BoxDecoration(
                          color: Colors.black,
                          border: Border.all(color: Colors.white, width: 2),
                          borderRadius: BorderRadius.circular(12),
                        ),
                        clipBehavior: Clip.hardEdge,
                        child: _imageUrl != null
                            ? Image.network(_imageUrl!, fit: BoxFit.cover, errorBuilder: (_, __, ___) {
                                return Image.asset('assets/images/placeholder.png', fit: BoxFit.cover);
                              })
                            : (_capturedFaceBytes != null
                                ? Image.memory(_capturedFaceBytes!, fit: BoxFit.cover)
                                : Image.asset('assets/images/placeholder.png', fit: BoxFit.cover)),
                      ),
                      const SizedBox(width: 12),
                      Column(
                        crossAxisAlignment: CrossAxisAlignment.start,
                        children: [
                          Text('ID: $_userId', style: const TextStyle(color: Colors.white, fontWeight: FontWeight.bold)),
                          Text('Name: $_userName', style: const TextStyle(color: Colors.white, fontWeight: FontWeight.bold)),
                          const SizedBox(height: 6),
                          Text(_status, style: const TextStyle(color: Colors.yellowAccent)),
                        ],
                      )
                    ],
                  ),
                ),
              ],
            );
          } else {
            return const Center(child: CircularProgressIndicator());
          }
        },
      ),
    );
  }
}
